import json
import boto3
import pandas as pd
from io import StringIO
import datetime 
import re
import os
import string
import nltk
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import time
nltk.data.path.append("/tmp")
nltk.download('stopwords', download_dir="/tmp")
nltk.download('vader_lexicon', download_dir="/tmp")


#call s3 bucket
client = boto3.client('s3')


def lambda_handler(event, context):
    
    #define bucket_name and object _name
    bucket_name = event['Records'][0]['s3']['bucket']['name']
    object_name = event['Records'][0]['s3']['object']['key']
        
    #create a df from the object
    response = client.get_object(Bucket=bucket_name, Key=object_name)
    df = pd.read_csv(response["Body"])
    
    #drop na values
    df = df.dropna()
    
    #modify time format
    df['date_time'] = df['Time Created'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S%z'))
    df['dt'] = df['date_time'].apply(lambda x: x.strftime('%m/%d/%Y'))
    df['tmp'] = df['date_time'].apply(lambda x: x.strftime('%H:%M:%S'))
    
    #delete commas
    df["Tweet"] = df["Tweet"].str.replace(",", " ")
    
    #drop columns
    df = df.drop(['Time Created','Number_of_Likes','date_time', 'Unnamed: 0','Unnamed: 0.1', 'Unnamed: 0.1.1.'], axis=1, errors= 'ignore')
    
    #drop duplicates and reset index
    df = df.drop_duplicates(subset=["Tweet"])
    df = df.reset_index(drop=True)

    #lowercase the tweets
    df["cleaned tweet"] = df["Tweet"].str.lower()
    
    #remove urls
    df["cleaned tweet"] = df["cleaned tweet"].str.replace(r'\s*https?://\S+(\s+|$)', ' ').str.strip()
    
    
    #set the stopwords language to english
    stop_words = set(stopwords.words("english"))
    
    #add stop words
    stop_words.add("ukraine")
    stop_words.add("russia")
    stop_words.add("standwithukraine")
    stop_words.add("ukrainerussiawar")
    stop_words.add("rt")
    
    #remove stopwords
    def remove_stopwords(text, stopwords = stop_words):
    #initialise list 
        result = []
        for word in text.split(): # tokenise with the split function 
        #remove stopwords
            if word not in stopwords:
                result.append(word)
        return result

    #apply it to the dataframe
    df["cleaned tweet"] = df["cleaned tweet"].apply(remove_stopwords)
    
    #remove emojis
    EMOJI_PATTERN = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    def remove_emojis(text):
      text = EMOJI_PATTERN.sub(r'', text)
      return text
    
    #apply it to the dataframe
    df["cleaned tweet"] = df["cleaned tweet"].apply(remove_emojis)
    
    #add sentiment
    sia = SentimentIntensityAnalyzer()
    def sentiment(text):
        return sia.polarity_scores(text)
    
    #apply sentiment
    df["cleaned tweet text"] = df["cleaned tweet"].apply(lambda x: ' '.join(x))
    df["polarity"] = df["cleaned tweet text"].apply(sentiment)
    
    #split columns
    df["pos"] = df["polarity"].apply(lambda x: x["pos"])
    df["neg"] = df["polarity"].apply(lambda x: x["neg"])
    df["neu"] = df["polarity"].apply(lambda x: x["neu"])
    df["compound"] = df["polarity"].apply(lambda x: x["compound"])
    
    #classify sentiment
    def classification(compound):
        if compound >= 0.05:
            return "Positive"
        elif compound <= -0.05:
            return "Negative"
        else:
            return "Neutral"
    
    #apply classification
    df["sentiment"] = df["compound"].apply(classification)
    
    #lowercase df column names
    df.rename(columns = {'Number_of_Retweets':'number_of_retweets', 'Tweet':'tweet'}, inplace = True)
    
    #drop unnecessary columns
    df = df.drop(['polarity', 'cleaned tweet'], axis=1)
     
    #rename columns
    df.rename(columns = {'Number_of_Retweets':'number_of_retweets', 'Tweet':'tweet', 'cleaned tweet text':'cleaned_tweet_text'}, inplace = True)
    
    #define filename
    csv_time = time.strftime("%Y%m%d%H%M")
    csv_name = "finalcleantweets" + csv_time + '.csv'

    # already created on S3
    bucket = 'finaltweepyclean' 
    
    # call s3 bucket
    s3 = boto3.resource('s3')
    
    #save it into a buffer
    csv_buffer = StringIO()
    df.to_csv(csv_buffer)
    s3.Object(bucket, csv_name).put(Body=csv_buffer.getvalue())
    
    

